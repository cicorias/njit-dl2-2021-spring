{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3. Text Preprocessing.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"9Juncx0qoaFn"},"source":["!pip install d2l==0.16.1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"origin_pos":2,"tab":["pytorch"],"id":"7Rmm3GoQn6Pc","executionInfo":{"status":"ok","timestamp":1613660036341,"user_tz":300,"elapsed":3183,"user":{"displayName":"Ioannis Koutis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUpO_Yrmrro3LHDUXR4Z71Ig38y7Warh3Ph5dFgK0=s64","userId":"17239720917701380808"}}},"source":["import collections\n","from d2l import torch as d2l\n","import re"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":0,"id":"VgfLHLS2n6PU"},"source":["# Text Preprocessing\n"]},{"cell_type":"markdown","metadata":{"origin_pos":4,"id":"sH_3qtaKn6Pe"},"source":["## Reading the Dataset\n","\n","To get started we load text from H. G. Wells' [*The Time Machine*](http://www.gutenberg.org/ebooks/35).\n","This is a fairly small corpus of just over 30000 words, but for the purpose of what we want to illustrate this is just fine.\n","More realistic document collections contain many billions of words.\n","The following function reads the dataset into a list of text lines, where each line is a string.\n","For simplicity, here we ignore punctuation and capitalization.\n"]},{"cell_type":"code","metadata":{"origin_pos":5,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"gdF2BN1Sn6Pe","executionInfo":{"status":"ok","timestamp":1613660094646,"user_tz":300,"elapsed":526,"user":{"displayName":"Ioannis Koutis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUpO_Yrmrro3LHDUXR4Z71Ig38y7Warh3Ph5dFgK0=s64","userId":"17239720917701380808"}},"outputId":"c5a12305-7c77-47b5-fc75-c12cb185ab2b"},"source":["#@save\n","d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',\n","                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n","\n","def read_time_machine():  #@save\n","    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n","    with open(d2l.download('time_machine'), 'r') as f:\n","        lines = f.readlines()\n","    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n","\n","lines = read_time_machine()\n","print(f'# text lines: {len(lines)}')\n","print(lines[0])\n","print(lines[10])"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading ../data/timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...\n","# text lines: 3221\n","the time machine by h g wells\n","twinkled and his usually pale face was flushed and animated the\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":6,"id":"w1QsC0Wzn6Pg"},"source":["## Step 1 - Tokenization\n","\n","The following `tokenize` function\n","takes a list (`lines`) as the input,\n","where each list is a text sequence (e.g., a text line).\n","Each text sequence is split into a list of tokens.\n","A *token* is the basic unit in text.\n","In the end,\n","a list of token lists are returned,\n","where each token is a string.\n"]},{"cell_type":"code","metadata":{"origin_pos":7,"tab":["pytorch"],"colab":{"base_uri":"https://localhost:8080/"},"id":"afYsvqZHn6Pg","executionInfo":{"status":"ok","timestamp":1613660178031,"user_tz":300,"elapsed":293,"user":{"displayName":"Ioannis Koutis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUpO_Yrmrro3LHDUXR4Z71Ig38y7Warh3Ph5dFgK0=s64","userId":"17239720917701380808"}},"outputId":"497587bc-c5a9-450b-f324-20aaf7d1651e"},"source":["def tokenize(lines, token='word'):  #@save\n","    \"\"\"Split text lines into word or character tokens.\"\"\"\n","    if token == 'word':\n","        return [line.split() for line in lines]\n","    elif token == 'char':\n","        return [list(line) for line in lines]\n","    else:\n","        print('ERROR: unknown token type: ' + token)\n","\n","tokens = tokenize(lines)\n","for i in range(11):\n","    print(tokens[i])"],"execution_count":4,"outputs":[{"output_type":"stream","text":["['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n","[]\n","[]\n","[]\n","[]\n","['i']\n","[]\n","[]\n","['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n","['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n","['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":8,"id":"3L7XKLkbn6Pg"},"source":["## Step 2 - Build Vocabulary\n","\n","The string type of the token is inconvenient to be used by models, which take numerical inputs.\n","Now let us build a dictionary, often called *vocabulary* as well, to map string tokens into numerical indices starting from 0.\n","To do so, we first count the unique tokens in all the documents from the training set,\n","namely a *corpus*,\n","and then assign a numerical index to each unique token according to its frequency.\n","Rarely appeared tokens are often removed to reduce the complexity.\n","Any token that does not exist in the corpus or has been removed is mapped into a special unknown token “&lt;unk&gt;”.\n","We optionally add a list of reserved tokens, such as\n","“&lt;pad&gt;” for padding,\n","“&lt;bos&gt;” to present the beginning for a sequence, and “&lt;eos&gt;” for the end of a sequence.\n"]},{"cell_type":"code","metadata":{"origin_pos":9,"tab":["pytorch"],"id":"0AdZIPbZn6Ph","executionInfo":{"status":"ok","timestamp":1613660529099,"user_tz":300,"elapsed":297,"user":{"displayName":"Ioannis Koutis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUpO_Yrmrro3LHDUXR4Z71Ig38y7Warh3Ph5dFgK0=s64","userId":"17239720917701380808"}}},"source":["class Vocab:  #@save\n","    \"\"\"Vocabulary for text.\"\"\"\n","    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n","        if tokens is None:\n","            tokens = []\n","        if reserved_tokens is None:\n","            reserved_tokens = [] \n","        # Sort according to frequencies\n","        counter = count_corpus(tokens)\n","        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n","                                  reverse=True)\n","        # The index for the unknown token is 0\n","        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens\n","        uniq_tokens += [token for token, freq in self.token_freqs\n","                        if freq >= min_freq and token not in uniq_tokens]\n","        self.idx_to_token, self.token_to_idx = [], dict()\n","        for token in uniq_tokens:\n","            self.idx_to_token.append(token)\n","            self.token_to_idx[token] = len(self.idx_to_token) - 1\n","\n","    def __len__(self):\n","        return len(self.idx_to_token)\n","\n","    def __getitem__(self, tokens):\n","        if not isinstance(tokens, (list, tuple)):\n","            return self.token_to_idx.get(tokens, self.unk)\n","        return [self.__getitem__(token) for token in tokens]\n","\n","    def to_tokens(self, indices):\n","        if not isinstance(indices, (list, tuple)):\n","            return self.idx_to_token[indices]\n","        return [self.idx_to_token[index] for index in indices]\n","\n","def count_corpus(tokens):  #@save\n","    \"\"\"Count token frequencies.\"\"\"\n","    # Here `tokens` is a 1D list or 2D list\n","    if len(tokens) == 0 or isinstance(tokens[0], list):\n","        # Flatten a list of token lists into a list of tokens\n","        tokens = [token for line in tokens for token in line]\n","    return collections.Counter(tokens)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"origin_pos":10,"id":"sM20g7kHn6Pi"},"source":["We construct a vocabulary using the time machine dataset as the corpus. \n","Then we print the first few frequent tokens with their indices.\n"]},{"cell_type":"code","metadata":{"origin_pos":11,"tab":["pytorch"],"id":"bQjcOpbCn6Pi","outputId":"afc55025-7f86-4308-b366-d466378fa1a7"},"source":["vocab = Vocab(tokens)\n","print(list(vocab.token_to_idx.items())[:10])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":12,"id":"-98_fO8qn6Pi"},"source":["Now we can convert each text line into a list of numerical indices.\n"]},{"cell_type":"code","metadata":{"origin_pos":13,"tab":["pytorch"],"id":"GOeOhpwSn6Pi","outputId":"02e98ea1-a945-4d0b-ebaa-92f556dbc803"},"source":["for i in [0, 10]:\n","    print('words:', tokens[i])\n","    print('indices:', vocab[tokens[i]])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["words: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n","indices: [1, 19, 50, 40, 2183, 2184, 400]\n","words: ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n","indices: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"origin_pos":14,"id":"R_KtpFzon6Pj"},"source":["## Putting All Things Together\n","\n","Using the above functions, we package everything into the `load_corpus_time_machine` function, which returns `corpus`, a list of token indices, and `vocab`, the vocabulary of the time machine corpus.\n","The modifications we did here are:\n","i) we tokenize text into characters, not words, to simplify the training in later sections;\n","ii) `corpus` is a single list, not a list of token lists, since each text line in the time machine dataset is not necessarily a sentence or a paragraph.\n"]},{"cell_type":"code","metadata":{"origin_pos":15,"tab":["pytorch"],"id":"5YuusLqjn6Pj","outputId":"8484ade1-139e-4188-f319-07e7eda56bf3"},"source":["def load_corpus_time_machine(max_tokens=-1):  #@save\n","    \"\"\"Return token indices and the vocabulary of the time machine dataset.\"\"\"\n","    lines = read_time_machine()\n","    tokens = tokenize(lines, 'char')\n","    vocab = Vocab(tokens)\n","    # Since each text line in the time machine dataset is not necessarily a\n","    # sentence or a paragraph, flatten all the text lines into a single list\n","    corpus = [vocab[token] for line in tokens for token in line]\n","    if max_tokens > 0:\n","        corpus = corpus[:max_tokens]\n","    return corpus, vocab\n","\n","corpus, vocab = load_corpus_time_machine()\n","len(corpus), len(vocab)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(170580, 28)"]},"metadata":{"tags":[]},"execution_count":7}]}]}