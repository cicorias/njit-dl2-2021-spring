{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5. autograd.ipynb","provenance":[],"authorship_tag":"ABX9TyN8YuPvpOS4IeJovH2ZqHnb"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UCDDycpzBya7"},"source":["## **Automatic Computation of Gradients**\r\n","\r\n","Let's take a look at how PyTorch can compute gradients"]},{"cell_type":"code","metadata":{"id":"S68ua3N5Fd12","executionInfo":{"status":"ok","timestamp":1611341582047,"user_tz":300,"elapsed":3066,"user":{"displayName":"Ioannis Koutis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUpO_Yrmrro3LHDUXR4Z71Ig38y7Warh3Ph5dFgK0=s64","userId":"17239720917701380808"}}},"source":["import torch"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hSa46v75Ff_B","executionInfo":{"status":"ok","timestamp":1611341583210,"user_tz":300,"elapsed":552,"user":{"displayName":"Ioannis Koutis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUpO_Yrmrro3LHDUXR4Z71Ig38y7Warh3Ph5dFgK0=s64","userId":"17239720917701380808"}},"outputId":"1671b548-4ea9-4a77-f68e-97eacdf03c3d"},"source":["x = torch.tensor(2., requires_grad = True)\r\n","y = torch.tensor(3., requires_grad = True)\r\n","\r\n","f = 3*x**3 - y**2\r\n","print(f)\r\n","\r\n","\r\n","# This computes the gradient\r\n","f.backward()\r\n","\r\n","# The derivative of f with respect to a is 9*x^2   -- Let's see if this is calculated correctly at x==2\r\n","# We expect it to be 9*2^2 = 36\r\n","\r\n","print(x.grad)   \r\n","\r\n","# The derivative of f with respect to a is -2*y   -- Let's see if this is calculated correctly at y==3\r\n","# We expect it to be -2*3 = -6\r\n","\r\n","print(y.grad)   \r\n","\r\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["tensor(15., grad_fn=<SubBackward0>)\n","tensor(36.)\n","tensor(-6.)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0HapP5eRA_Qj","executionInfo":{"status":"ok","timestamp":1611341585926,"user_tz":300,"elapsed":376,"user":{"displayName":"Ioannis Koutis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUpO_Yrmrro3LHDUXR4Z71Ig38y7Warh3Ph5dFgK0=s64","userId":"17239720917701380808"}}},"source":["# Notice how our tensors are now vectors\r\n","\r\n","\r\n","x = torch.tensor([2., 3.], requires_grad=True)\r\n","y = torch.tensor([6., 4.], requires_grad=True)\r\n","\r\n","\r\n","\r\n","F = 3*x**3 - y**2\r\n","\r\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kWTRq3hPKW0r"},"source":["#### **A small reference to Jacobians**\r\n","\r\n","Now our function F is also a vector, not a scalar as before. Specifically\r\n","we have \r\n","\r\n","$$  F = \r\n"," \\begin{pmatrix}\r\n"," 3 x_1^3 - y_1^2 \\\\\r\n"," 3 x_2^3 - y_2^2\r\n","\\end{pmatrix}\r\n","$$\r\n","\r\n","Here $x_i$ is the entry $i$ of the vector $x$, and similarly for $y_i$.\r\n","\r\n","\r\n","Formally, the generalization of the 'gradient' of F with respect to the vector $x$ is called a Jacobian and looks like this:\r\n","$$  J = \r\n"," \\begin{pmatrix}\r\n"," \\frac{\\partial{F_1}}{\\partial x_1} & \\frac{\\partial{F_1}}{\\partial x_2} \\\\\r\n"," \\frac{\\partial{F_2}}{\\partial x_1} & \\frac{\\partial{F_2}}{\\partial x_2} \r\n","\\end{pmatrix}\r\n","$$\r\n","\r\n","Here $F_i$ is the entry $i$ of the vector $F$.If we do the calculations we get:\r\n","\r\n","$$\r\n"," \\begin{pmatrix}\r\n"," 9x_1^2 & 0 \\\\\r\n"," 0 & 9x_2^2\r\n","\\end{pmatrix}\r\n","$$\r\n","\r\n","More generally, the gradient of a function with $m$ entries with respect to a vector of $n$ entries, will be a Jacobian matrix of size $m \\times n$. In general, all entries can be non-zero (more on the assignment). \r\n","\r\n","\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":306},"id":"n803En6pBiWr","executionInfo":{"status":"error","timestamp":1611188097986,"user_tz":300,"elapsed":405,"user":{"displayName":"Ioannis Koutis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUpO_Yrmrro3LHDUXR4Z71Ig38y7Warh3Ph5dFgK0=s64","userId":"17239720917701380808"}},"outputId":"bf18e25a-9a5d-4e34-e24a-6e78e97988d0"},"source":["F.backward()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-99ea150b60f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"]}]},{"cell_type":"markdown","metadata":{"id":"_bqmTXl7Cjw3"},"source":["Here 'backward' will not produce the entire Jacobian as one might expect. The reason is practical. The Jacobian will be a very big matrix,  and not all gradients are needed by the solvers. So, they are not computed. \r\n","\r\n","We only need **linear combinations** of these gradients (i.e. their weighted sums). So, here 'backward' requires another vector tensor argument $T$, and what is computed internally is the product $J^T v$. In fact, this Jacobian-vector products simply computes the chain rule with such 'vector' functions. \r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"O4GAw5PlBtfz","executionInfo":{"status":"ok","timestamp":1611341593197,"user_tz":300,"elapsed":356,"user":{"displayName":"Ioannis Koutis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUpO_Yrmrro3LHDUXR4Z71Ig38y7Warh3Ph5dFgK0=s64","userId":"17239720917701380808"}}},"source":["T = torch.tensor([1., 1.])\r\n","F.backward(gradient=T)\r\n","\r\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dazu5-F-DOVk","executionInfo":{"status":"ok","timestamp":1611189547969,"user_tz":300,"elapsed":418,"user":{"displayName":"Ioannis Koutis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUpO_Yrmrro3LHDUXR4Z71Ig38y7Warh3Ph5dFgK0=s64","userId":"17239720917701380808"}},"outputId":"1d15d856-0a31-4cfe-83c7-ea1a2b60b768"},"source":["print(x.grad)\r\n","print(y.grad)\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([36., 81.])\n","tensor([-12.,  -8.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PSXkGduiTzCA","executionInfo":{"status":"ok","timestamp":1611276968085,"user_tz":300,"elapsed":283,"user":{"displayName":"Ioannis Koutis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUpO_Yrmrro3LHDUXR4Z71Ig38y7Warh3Ph5dFgK0=s64","userId":"17239720917701380808"}},"outputId":"4b2a7721-e085-4d0c-d7eb-45b63bf39f7c"},"source":["# now let's try to assess the gradient of F\r\n","print(F.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["None\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n","  \n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"JivXf9uzY-wg"},"source":["### Automatic gradients of more complicated functions\r\n","\r\n","The automatic gradient computation supports multiple basic functions and all their combinations. But **importantly** these functions need to be computed\r\n","using their pytorch versions. For example:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mgzQuIpIZN-Q","executionInfo":{"status":"ok","timestamp":1611277106553,"user_tz":300,"elapsed":314,"user":{"displayName":"Ioannis Koutis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUpO_Yrmrro3LHDUXR4Z71Ig38y7Warh3Ph5dFgK0=s64","userId":"17239720917701380808"}},"outputId":"1413bb87-0735-468c-e094-de41871067f9"},"source":["x = torch.tensor(2., requires_grad = True)\r\n","y = torch.tensor(3., requires_grad = True)\r\n","\r\n","f = 3*torch.cos(x) - torch.sin(y)**2\r\n","\r\n","# This computes the gradient\r\n","f.backward()\r\n","\r\n","# The derivative of f with respect to a is 9*x^2   -- Let's see if this is calculated correctly at x==2\r\n","# We expect it to be 9*2^2 = 36\r\n","\r\n","print(x.grad)   \r\n","\r\n","# The derivative of f with respect to a is -2*y   -- Let's see if this is calculated correctly at y==2\r\n","# We expect it to be -2*3 = -6\r\n","\r\n","print(y.grad) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor(-2.7279)\n","tensor(0.2794)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MW0zksSfZd7R"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_s8tozcneGrQ"},"source":["### Side note: How to be frugal with derivatives\r\n","\r\n","We can exclude some parameters from derivative computation, a fact\r\n","that as we will see is useful when we are done training a model and\r\n","we just want to evaluate it on new points. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MUySn7sZebep","executionInfo":{"status":"ok","timestamp":1611191147510,"user_tz":300,"elapsed":396,"user":{"displayName":"Ioannis Koutis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUpO_Yrmrro3LHDUXR4Z71Ig38y7Warh3Ph5dFgK0=s64","userId":"17239720917701380808"}},"outputId":"264dbcdb-3e20-4d49-8d54-f263ff166ea4"},"source":["x = torch.tensor(2., requires_grad = True)\r\n","y = torch.tensor(3., requires_grad = False)\r\n","\r\n","f = 3*torch.cos(x) - torch.sin(y)**2\r\n","\r\n","# This computes the gradient\r\n","f.backward()\r\n","\r\n","# The derivative of f with respect to a is 9*x^2   -- Let's see if this is calculated correctly at x==2\r\n","# We expect it to be 9*2^2 = 36\r\n","\r\n","print(x.grad)   \r\n","\r\n","# The derivative of f with respect to a is -2*y   -- Let's see if this is calculated correctly at y==2\r\n","# We expect it to be -2*3 = -6\r\n","\r\n","print(y.grad) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor(-2.7279)\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p2oUTj_aedna"},"source":[""],"execution_count":null,"outputs":[]}]}