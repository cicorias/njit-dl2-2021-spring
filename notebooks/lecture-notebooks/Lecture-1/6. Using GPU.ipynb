{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Using CUDA ","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO7of8CpM7KrS1X9ZpVaM6w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TO_3Oef48HaY","executionInfo":{"status":"ok","timestamp":1611277564068,"user_tz":300,"elapsed":3892,"user":{"displayName":"Ioannis Koutis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUpO_Yrmrro3LHDUXR4Z71Ig38y7Warh3Ph5dFgK0=s64","userId":"17239720917701380808"}},"outputId":"7d18c0c4-0526-4394-b9ed-ec0254ad0291"},"source":["import numpy as np\r\n","import torch\r\n","import time\r\n","\r\n","# All machines have CPUs but some machines have also GPUs\r\n","# Having a GPU can make a huge difference\r\n","\r\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n","print(device)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3A19YSp4Gb1r","executionInfo":{"status":"ok","timestamp":1611277804088,"user_tz":300,"elapsed":1972,"user":{"displayName":"Ioannis Koutis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUpO_Yrmrro3LHDUXR4Z71Ig38y7Warh3Ph5dFgK0=s64","userId":"17239720917701380808"}}},"source":["# Generate some random matrices. These are numpy, so they 'live' on the CPU\r\n","\r\n","n = 10000\r\n","a = np.random.random([n,n])\r\n","b = np.random.random([n,n])"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Iu2HEE2GpE5","executionInfo":{"status":"ok","timestamp":1611277816975,"user_tz":300,"elapsed":11813,"user":{"displayName":"Ioannis Koutis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUpO_Yrmrro3LHDUXR4Z71Ig38y7Warh3Ph5dFgK0=s64","userId":"17239720917701380808"}}},"source":["# But we can put them on the GPU\r\n","\r\n","Ta  = torch.tensor(a).float().to(device)\r\n","Tb  = torch.tensor(b).float().to(device)\r\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0gEt2XjhIte6","executionInfo":{"status":"ok","timestamp":1611277859092,"user_tz":300,"elapsed":24268,"user":{"displayName":"Ioannis Koutis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhUpO_Yrmrro3LHDUXR4Z71Ig38y7Warh3Ph5dFgK0=s64","userId":"17239720917701380808"}},"outputId":"40c08815-3239-4ba6-adcd-9456d4374c82"},"source":["# Let's check the runtimes\r\n","\r\n","start = time.time()\r\n","Tc = torch.matmul(Ta,Tb)\r\n","end = time.time()\r\n","print(end-start)\r\n","\r\n","# now let's put those back in the CPU and check again runtimes\r\n","\r\n","Ta = torch.Tensor.cpu(Ta)\r\n","Tb = torch.Tensor.cpu(Tb)\r\n","\r\n","start = time.time()\r\n","Tc = torch.matmul(Ta,Tb)\r\n","end = time.time()\r\n","print(end-start)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["0.023424863815307617\n","23.108078002929688\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X7OlrzWuJ_UJ"},"source":["# of course, most of the times we won't have to move data between CPU and GPU "],"execution_count":null,"outputs":[]}]}